# Notes: March 20, 2020
Today, I'm moving forward with the idea of versioning paths. Goals for the day (and looking ahead):

1. Think about the various operations on paths that have to be tracked, and how they will interact with versioning of actual file contents.
2. Summarize the rules for deciding when a command must rerun. Let's try to avoid checking to see if the build is in the final state from its previous run; just check if the initial state matches the initial state on the last build.
3. Sketch out a few scenarios to test the versioning.
4. Assuming everything looks okay, implement path versioning in parallel with the existing tracing code. If these can be unified (e.g. paths are just another kind of artifact) avoid migrating over the file tracking at first, since this introduced some bugs before, especially related to dup and file descriptor handling.
5. Add visualization output for path tracking, so we can sanity check the simple test cases.
6. Migrate the existing artifact tracking to play nicely with path tracking.

## Tracking Path Operations
There are (at least) two options for how we can think about paths as entities in this system:
1. A path is an entry in a directory
2. A path is just a string that resolves (or does not resolve) to some file

If we track at the level of entries in directories, we'll have to recurse into parent directories as well. An operation on a path `/foo/bar` depends on the `bar` entry in the directory referenced via `/foo`, which in turn depends on the `foo` entry in the directory `/`. The downside of this is that we'll have a lot more dependencies to keep track of, but it correctly associates operations on the same directory entry even when they use different path strings. Here's an example:

/foo is a directory
/foo/bar is a directory
/baz is a directory
/baz/egads is a soft link to /foo/bar

If a command creates file /foo/bar/X, then unlinks /baz/egads/X, we'll correctly associate these two operations as referring to the same directory entry. That's because the inode numbers for /foo/bar and /baz/egads are the same. If we used the path string representation we couldn't easily associate these operations as manipulating the same thing. When we operate on a file the actually exists we can use inode number, but hard links refer to the same inode number while being independent entries in different directories. We don't want to conflate sharing of a directory entry and sharing of an actual file's contents. Note that tracking paths as directory entries works for relative paths too; `/foo/bar/..` has the same inode number as `/baz/egads/..` (both refer to `/foo`).

Do we really need to chase paths up to root, or can we just break a path into its directory and entry parts? As an example, consider a command that creates the file `/foo/bar/baz`:

### Recursive entry tracking
Creating the file `/foo/bar/baz` creates the entry `baz` in the directory referenced by `/foo/bar`. This in turn creates a dependency on the `bar` entry in the directory `/foo`, and again that depends on the entry `foo` in the directory `/`.

It may be easier to think of this in the reverse order, though. The directory `/` is a fixed starting point (ignoring chroot for now). It resolves to an inode for a directory, which in turn has a `foo` entry. At each level, the command depends on these directory entries. The command does *not* depend on the directories themselves, since that implies we care about their entire contents, whereas here we just care about a specific entry.

When we create a dependency on a directory entry, do we depend on what that entry resolves to, or just on its presence or absence? It's tempting to just depend on presence or absence, but it seems necessary to also track what that entry resolves to. If a command creates a directory entry with the same name twice (presumably unlinking between those two), then other commands that use those entries will depend on which of the two linked files is currently staged in.

### One-level entry tracking
Instead of recursing up to `/`, we stop at one level. Creating the file `/foo/bar/baz` creates two path-related edges in the build graph: `/foo/bar` resolves to some directory, and the entry `baz` is created in that directory. This could be made to handle references to directories accessed via different paths, since the directory that's modified will be identified by inode number. I don't love that this creates a new kind of dependency (e.g. some path resolves to some directory) but it's not totally unreasonable.

***This approach seems to have no particular upsides, and some possible downsides. I'll opt for recursive tracking for now.***

### A third approach: track directories only
Instead of thinking of directory entries as separate entities a command can depend on, we could just log these as operations on directories themselves. That has the added benefit that every entity in the system can be identified by inode number (except anonymous pipes and sockets, but dealing with those is already straightforward because they don't exist outside the scope of a single build).

Adding a file to a directory is just a modification of that directory. This would have a downside though: it serializes operations that are actually independent. If I add a file to a directory, it doesn't really matter what other files are there, as long as there isn't already a file with the same name. To get around this, we can think of pushing dependencies up the sequence of versions (to old versions). Say the most recent version of a directory was created by adding an entry named "foo". If a command is adding an entry named "bar", there's no dependency on the most recent version, so the dependency edge can move back a version. This can continue until we hit a a version that could interact with this operation (e.g. unlinking an entry named "bar", changing directory permissions, etc.). If the dependency can be pushed all they way back before the earliest version, that identifies this as a requirement that was satisfied by the state of the system prior to the build.

There are some complications to worry about, but this could also be generalized to files themselves. If two commands open a file in append mode, their operations create new versions but dependencies do not serialize those operations. To make this work, we would need to know if commands have any happens-before relationships. Most of the operations to synchronize commands would be cases where we collapse the commands (communicating with signals, through file contents, etc.) but forkâ€“exec and wait would be relevant. We couldn't push an append operation's prior-version dependency past a version that was created before that command ran. This is definitely more complex than we want to deal with right now, but the idea has some general applications that would be useful.

A simpler, useful benefit of this approach shows up with file tracking. The `gcc` command creates a `.s` temp file, then `as` opens it with `O_CREAT | O_TRUNC`. We have baked-in logic to not create a dependency on the earlier version, but this would subsume that. An open with `O_CREAT | O_TRUNC` can't depend on anything (other than write permissions, I guess), so the dependency edge falls off the beginning of the version list and becomes a pre-build dependency.

***This approach could be combined with one-level or recursive tracking. It's not really an alternative, but a way to simplify some details of either tracking method.***
