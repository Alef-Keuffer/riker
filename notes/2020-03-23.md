# Notes: March 23, 2020
Picking up where I left off on Friday, I've made a few decisions:

1. Dependencies on paths will be tracked recursively from the root directory.
2. Operations on paths (e.g. creating, renaming, and unlinking) will be modifications to the containing directory.
3. Dependency edges will be pushed up to the earliest-possible version. This avoids creating serialized operations on directories that are actually independent. Dependencies that fall off the earliest version become pre-build dependencies.

There are a few questions that come to mind:

**What do we do with relative paths?** 
The easiest approach is to convert them to absolute paths. Do we lose anything by doing that? Maybe not, since commands can and do convert relative paths to absolute paths before making syscalls. Let's assume we always convert them to absolute paths.

**How do we order versions?** 
Moving dependencies earlier means that file and path versions are no longer totally ordered. That seems okay (good, even) but what is the best way to show that in the build graph? We want it to be clear because that's currently the only mechanism for checking the accuracy of the tracing operations. Some kind of happens-before graph of versions seems reasonable. Getting graphviz to lay these out close together will be interesting, but should be possible with edge weights.

**What information is tracked with a dependency edge?**
When we are deciding whether a dependency edge can migrate to an earlier version, we need to know (a) what was changed in the version we may migrate past, and (b) what the dependency edge actually depends on.

## Rules for path tracking
Interactions with paths usually come with some additional operation (e.g. creating a file, opening a file). I'm thinking through how these operations translate to path dependencies/updates.

### Opening a file
A command opens the file `/a/b/c` in read-write mode without specifying the O_CREAT flag. If this operation succeeds, this creates a recursive set of dependencies:
- Read entry `a` of directory `/`, resolves to dir `/a/` (this final name is just shorthand. Results of resolving a path will actually be identified by inode number)
- Read entry `b` of directory `/a/`, resolves to dir `/a/b/`
- Read entry `c` of directory `/a/b/`, resolves to file `/a/b/c`
- Opening file `/a/b/c` read-write succeeds

What if the operation fails? The easiest way to track this is to encode the specific failure, I think. The exact dependencies will depend on where the process fails:

**No `a` in directory `/`**
- Read entry `a` of directory `/`, does not resolve

**`/a` exists, but is not a directory**
- Read entry `a` of directory `/`, resolves to file `/a`
- Read entry `b` of file `/a`, fails

**No `b` in directory `/a/`**
- Read entry `a` of directory `/`, resolves to directory `/a/`
- Read entry `b` of directory `/a/`, does not resolve

**`/a/b` exists, but is not a directory**
- Read entry `a` of directory `/`, resolves to directory `/a/`
- Read entry `b` of directory `/a/`, resolves to file `/a/b`
- Read entry `c` of file `/a/b`, fails

**No `c` in directory `/a/b`:**
- Read entry `a` of directory `/`, resolves to dir `/a/`
- Read entry `b` of directory `/a/`, resolves to dir `/a/b/`
- Read entry `c` of directory `/a/b/`, does not resolve

Permission errors are also possible. Just as one illustrative case, say directory `/a/b` does not have execute permission for the current user:
- Read entry `a` of directory `/`, resolve to dir `/a/`
- Read entry `b` of directory `/a/`, resolves to dir `/a/b/`
- Read entry `c` of directory `/a/b/`, does not resolve

The question here is how we handle permissions. Any time we read from a directory (e.g. access one of its entries), we have a choice. Is there an explicit dependency on the directory's permission state, or do we just implicitly depend on having read access? In other words, do we want to record the output from `stat`ing this directory as part of its state? If not, we can treat a dependency like "read entry `c` from directory `/a/b/`" as having an implicit requirement that we are *able* to access that entry.

Of course, the way that access fails is observable to a command. If a command tries to access file `/a/b/c` it may be checking if the file exists. If it doesn't, it could create the file. The response would be quite different if the command discovers it does not have access to `/a/b/`. We can also distinguish those cases, since the syscall `open` will return `ENOENT` if the file doesn't exist, and `EACCESS` if we don't have access to some part of the path. On a future run, we'll need to look at this dependency and ask "If this command ran again, would its `open` call return the same error code?" That's a little trickier than recording permission data, but it makes more sense. We can check if a regular old `open` will work using the `access` call, but some other operations require us to encode the logic ourselves.

#### Information on Directory Depenency Edges
Given these cases, it seems that each depenency edge on a directory version has to track a few pieces of information:
1. What command has the dependency?
2. What directory and version is there a dependency on?
3. What access is being made to the directory? Options are:
  a. access a specific entry (reqires execute permission),
  b. list directory contents (requires read permission), or
  c. stat the directory (no permissions required)
4. What is the result of the access? It will either be success + the actual result (e.g. file resolved to, directory contents, or stat data), or failure with an error code.

#### Checking Directory Dependencies
Say a command depends on an access to the path `/a/b`, and this access succeeded on the previous build. We need to check:
1. Are we able to get the `a` entry from `/`? Let's say this succeeds and resolves to a directory. Call that directory D for now.
2. Does directory D have a `b` entry?

This is tricky, because we can't directly check D. We can identify it by its inode number, but we can't use an inode number to open it again later. Plus, inode numbers could change without any file contents or properties changing.

One approach we could take is to identify files and directories by inode during a build, but use paths across builds. So, we'll know a path for any file or directory the program accesses, and we can use these paths on a future build to check whether files or directories have changed.

An alternative that seems a bit more appealing is to identify directories and files by their provenance. When a command accesses path `/a/b/c`, we'd think of it as this chain:
Accessing `a` in root yielded directory D.
Accessing `b` in directory D yielded directory E.
Accessing `c` in directory E yielded file F.

So the file this finally resolves to is identified by the chain of accesses that get us there. This is appealing because we can always convert this chain of accesses back to a path, even if the command actually accessed the file using `openat` relative to a directory other than root.

## Misc Notes
To list a directory's contents, you must have read permissions on that directory.
To access a file in a directory, you must have execute permissions for that directory.
To stat a file, you need execute permissions in its containing directory. You do *not* need any permissions on the file itself.

## Alternate Approach
This path tracking approach is starting to feel suspiciously complicated. Possibly just as an exercise, I'll consider some alternate approaches below.

### Log-Based Tracing
Dramatically simplify tracing by just collecting a log of accesses and results. We're left with a totally-ordered sequence of operations performed by the entire build, each operation tagged with the command that performed it. We'd probably want to collapse repeated reads and writes from the same command in much the same way we already do. Regardless of how we handle that, we would definitely need to hold on to the checksums for read versions of files, and full copies of overwritten or removed files in case we need to re-stage them.

The more complex logic comes in during a rebuild. To decide whether or not to run a command, we have to look at each of the command's direct inputs and decide if those inputs (a) have changed, or (b) could change (if they are produced by some other command). If none of the inputs fall into either case, we do not need to run the command.

If we don't run a command, we'd mark its outputs as available, then proceed on to child commands. For each child command, we check its inputs as before. A child command that reads an output from its skipped parent should see that version as available.

This seems to fall back on a build graph, since we need to know if there is a path from a changed file to any particular command when deciding whether or not to run it. Does the ordering of operations help us? Maybe not, since ordering operations that are independent is just an extra constraint.
